{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8e70d-b1a8-4930-a620-ab1b101a14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score \n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report,precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fb1a9-23d4-4f6a-bbaf-964d95863fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('D:/Users/tonyn/Desktop/da_sci_4th/datathon/DATA/Base.csv')\n",
    "base_copy = base.copy()\n",
    "\n",
    "var1 = pd.read_csv('D:/Users/tonyn/Desktop/da_sci_4th/datathon/DATA/Variant I.csv')\n",
    "var1_copy = var1.copy()\n",
    "\n",
    "var2 = pd.read_csv('D:/Users/tonyn/Desktop/da_sci_4th/datathon/DATA/Variant II.csv')\n",
    "var2_copy = var2.copy()\n",
    "\n",
    "var3 = pd.read_csv('D:/Users/tonyn/Desktop/da_sci_4th/datathon/DATA/Variant III.csv')\n",
    "var3_copy = var3.copy()\n",
    "\n",
    "var4 = pd.read_csv('D:/Users/tonyn/Desktop/da_sci_4th/datathon/DATA/Variant IV.csv')\n",
    "var4_copy = var4.copy()\n",
    "\n",
    "var5 = pd.read_csv('D:/Users/tonyn/Desktop/da_sci_4th/datathon/DATA/Variant V.csv')\n",
    "var5_copy = var5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d044a93-f842-4e57-af50-bd3802a5cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EDA dataset preprocessing ###\n",
    "def EDA_dataset(df):\n",
    "    drop_col = ['payment_type', 'employment_status', 'prev_address_months_count', 'intended_balcon_amount', 'housing_status', 'days_since_request']\n",
    "    df.drop(columns = drop_col, inplace = True)\n",
    "\n",
    "    df = df[df['current_address_months_count'] >= 0]\n",
    "\n",
    "    df['bank_months_count'].replace({-1: 0}, inplace = True)\n",
    "\n",
    "    df = df[df['session_length_in_minutes'] >= 0]\n",
    "\n",
    "    df['proposed_credit_limit'] = df['proposed_credit_limit'].astype(int)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3226b1-da4d-4c7b-bcca-8c20b58a5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_copy = EDA_dataset(base_copy)\n",
    "var1_copy = EDA_dataset(var1_copy)\n",
    "var2_copy = EDA_dataset(var2_copy)\n",
    "var3_copy = EDA_dataset(var3_copy)\n",
    "var4_copy = EDA_dataset(var4_copy)\n",
    "var5_copy = EDA_dataset(var5_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a4cda-9f0f-4585-aa3a-751413a56e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot encoding for categorical variables####\n",
    "def one_hot(df):\n",
    "    object_cols = ['source', 'device_os']\n",
    "    df = pd.get_dummies(df, columns=object_cols, drop_first=True, dtype=int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642492d-a307-4464-a0a4-3a881aab0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_copy = one_hot(base_copy)\n",
    "var1_copy = one_hot(var1_copy)  \n",
    "var2_copy = one_hot(var2_copy)\n",
    "var3_copy = one_hot(var3_copy)\n",
    "var4_copy = one_hot(var4_copy)\n",
    "var5_copy = one_hot(var5_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526de55-142e-4565-a581-f04f4594d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "###100만개 데이터 중 10만개씩 sampling###\n",
    "def sample_data_stratified(df):\n",
    "    df.loc[(df['customer_age'] < 50) & (df['fraud_bool'] == 0), 'group'] = 0\n",
    "    df.loc[(df['customer_age'] < 50) & (df['fraud_bool'] == 1), 'group'] = 1\n",
    "    df.loc[(df['customer_age'] >= 50) & (df['fraud_bool'] == 0), 'group'] = 2\n",
    "    df.loc[(df['customer_age'] >= 50) & (df['fraud_bool'] == 1), 'group'] = 3\n",
    "    df['group'] = df['group'].astype(int)\n",
    "\n",
    "    X = df.drop(columns=['group'])\n",
    "    y = df['group']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42, stratify=y)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sam, X_test, y_train, y_test = sample_data_stratified(base_copy)\n",
    "var1_sam, X_test, y_train, y_test = sample_data_stratified(var1_copy)\n",
    "var2_sam, X_test, y_train, y_test = sample_data_stratified(var2_copy)\n",
    "var3_sam, X_test, y_train, y_test = sample_data_stratified(var3_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###100만개 데이터 중 10만개씩 sampling###\n",
    "def sample_data_stratified_2(df):\n",
    "    df.loc[(df['month'] <= 6) & (df['customer_age'] < 50) & (df['fraud_bool'] == 0), 'group'] = 0\n",
    "    df.loc[(df['month'] <= 6) & (df['customer_age'] < 50) & (df['fraud_bool'] == 1), 'group'] = 1\n",
    "    df.loc[(df['month'] <= 6) & (df['customer_age'] >= 50) & (df['fraud_bool'] == 0), 'group'] = 2\n",
    "    df.loc[(df['month'] <= 6) & (df['customer_age'] >= 50) & (df['fraud_bool'] == 1), 'group'] = 3\n",
    "    df.loc[(df['month'] > 6) & (df['customer_age'] < 50) & (df['fraud_bool'] == 0), 'group'] = 4\n",
    "    df.loc[(df['month'] > 6) & (df['customer_age'] < 50) & (df['fraud_bool'] == 1), 'group'] = 5\n",
    "    df.loc[(df['month'] > 6) & (df['customer_age'] >= 50) & (df['fraud_bool'] == 0), 'group'] = 6\n",
    "    df.loc[(df['month'] > 6) & (df['customer_age'] >= 50) & (df['fraud_bool'] == 1), 'group'] = 7\n",
    "    df['group'] = df['group'].astype(int)\n",
    "\n",
    "    X = df.drop(columns=['group'])\n",
    "    y = df['group']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42, stratify=y)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var4_sam, X_test, y_train, y_test = sample_data_stratified_2(var4_copy)\n",
    "var5_sam, X_test, y_train, y_test = sample_data_stratified_2(var5_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34758fb-7adb-4c20-a585-7fdfd8a07115",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Correlation Matrix Heatmap ###\n",
    "df = [base_copy, var1_copy, var2_copy, var3_copy, var4_copy, var5_copy]\n",
    "for df in df:\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    plt.figure(figsize=(35, 35))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,  \n",
    "        annot=True,          \n",
    "        cmap='coolwarm',     \n",
    "        fmt=\".2f\",          \n",
    "        linewidths=.5,      \n",
    "        cbar=True           \n",
    "    )\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Distribution of All Columns ###\n",
    "num_cols = len(base_copy.columns)\n",
    "x = 4\n",
    "y = (num_cols + x - 1) // x \n",
    "plt.figure(figsize=(x * 5, y * 4))\n",
    "for i, col in enumerate(base_copy.columns):\n",
    "    plt.subplot(y, x, i + 1)\n",
    "\n",
    "    if base_copy[col].nunique() < 5 and base_copy[col].dtype == 'int64':\n",
    "            sns.countplot(x=col, data=base_copy)\n",
    "            plt.xlabel(col, fontsize=10)\n",
    "            plt.ylabel('Count', fontsize=10)\n",
    "\n",
    "    else:\n",
    "        sns.histplot(base_copy[col], kde=True, bins=30)\n",
    "        plt.xlabel(col, fontsize=10)\n",
    "        plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('All Columns Distribution', y=1.02, fontsize=18) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fraud_bool이 1인 경우의 모든 컬럼 분포 시각화 ---\n",
    "fraud_data = base_copy[base_copy['fraud_bool'] == 1]\n",
    "\n",
    "# 사기 데이터셋에 대한 컬럼 수 계산 (동일할 수 있지만, 명시적으로 다시 계산)\n",
    "num_fraud_cols = len(fraud_data.columns)\n",
    "x_fraud = 4 # 동일한 레이아웃 유지\n",
    "y_fraud_rows = (num_fraud_cols + x_fraud - 1) // x_fraud \n",
    "\n",
    "plt.figure(figsize=(x_fraud * 5, y_fraud_rows * 4))\n",
    "for i, col in enumerate(fraud_data.columns):\n",
    "    plt.subplot(y_fraud_rows, x_fraud, i + 1)\n",
    "\n",
    "    # 이산형 변수 (카운트 플롯)와 연속형 변수 (히스토그램) 구분\n",
    "    if fraud_data[col].nunique() < 5 and fraud_data[col].dtype == 'int64':\n",
    "        sns.countplot(x=col, data=fraud_data)\n",
    "        plt.xlabel(col, fontsize=10)\n",
    "        plt.ylabel('Count', fontsize=10)\n",
    "    else:\n",
    "        sns.histplot(fraud_data[col], kde=True, bins=30)\n",
    "        plt.xlabel(col, fontsize=10)\n",
    "        plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('All Columns Distribution (for fraud_bool = 1)', y=1.02, fontsize=18) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc61f0a-8087-4e47-8b8e-948591341e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data splitting for modelingng\n",
    "def split_train_test(df):\n",
    "    df.loc[(df['customer_age'] < 50) & (df['fraud_bool'] == 0), 'group'] = 0\n",
    "    df.loc[(df['customer_age'] < 50) & (df['fraud_bool'] == 1), 'group'] = 1\n",
    "    df.loc[(df['customer_age'] >= 50) & (df['fraud_bool'] == 0), 'group'] = 2\n",
    "    df.loc[(df['customer_age'] >= 50) & (df['fraud_bool'] == 1), 'group'] = 3\n",
    "    df['group'] = df['group'].astype(int)\n",
    "\n",
    "    X = df.drop(columns=['fraud_bool', 'group'])\n",
    "    y = df['group']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42, stratify=y)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_base, x_test_base, y_train_base, y_test_base = split_train_test(base_sam)\n",
    "x_train_var1, x_test_var1, y_train_var1, y_test_var1 = split_train_test(var1_sam)\n",
    "x_train_var2, x_test_var2, y_train_var2, y_test_var2 = split_train_test(var2_sam)\n",
    "x_train_var3, x_test_var3, y_train_var3, y_test_var3 = split_train_test(var3_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_2(df):\n",
    "    X_train = df[df['month'] <= 6].drop(columns=['fraud_bool', 'group'])\n",
    "    y_train = df[df['month'] <= 6]['group']\n",
    "    X_test = df[df['month'] > 6].drop(columns=['fraud_bool', 'group'])\n",
    "    y_test = df[df['month'] > 6]['group']\n",
    "    \n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de515a0c-3a51-4cc3-9076-3455582a39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_var4, x_test_var4, y_train_var4, y_test_var4 = split_train_test_2(var4_sam)\n",
    "x_train_var5, x_test_var5, y_train_var5, y_test_var5 = split_train_test_2(var5_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce1ef7-aee5-48e1-8de9-a05b6d9078e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_training_smote(model_instance, X_train, y_train):\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(k_fold.split(X_train, y_train)):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('smote', SMOTE(random_state=42)), # SMOTE 인스턴스\n",
    "            ('classifier', model_instance)      # 모델 인스턴스 (하이퍼파라미터가 적용된)\n",
    "        ])\n",
    "        # 모델 학습\n",
    "        pipeline.fit(X_train_fold, y_train_fold)\n",
    "        y_val_proba = pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "        auc_score = roc_auc_score(y_val_fold, y_val_proba)\n",
    "        auc_scores.append(auc_score)\n",
    "\n",
    "        # 마지막 폴드의 결과 저장\n",
    "        if fold == k_fold.get_n_splits() - 1:\n",
    "            last_fold_y_val = y_val_fold\n",
    "            last_fold_y_proba = y_val_proba\n",
    "            last_trained_pipeline = pipeline # 마지막 학습된 파이프라인 저장\n",
    "\n",
    "    return auc_scores, last_fold_y_val, last_fold_y_proba, last_trained_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bdf20-9542-4532-b673-2b66d9b8f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LightGBM 베이지안 최적화를 위한 하이퍼파라미터 공간 정의\n",
    "\n",
    "lgb_space = [\n",
    "Integer(100, 1000, name='n_estimators'),              # 부스팅 라운드 수\n",
    "Real(0.01, 0.3, name='learning_rate', prior='log-uniform'),  # 학습률\n",
    "Integer(20, 300, name='num_leaves'),                  # 리프 노드의 최대 개수\n",
    "Integer(3, 15, name='max_depth'),                     # 트리의 최대 깊이\n",
    "Real(0.5, 1.0, name='feature_fraction'),             # 각 트리에서 사용할 피처 비율\n",
    "Real(0.5, 1.0, name='bagging_fraction'),             # 각 트리에서 사용할 데이터 비율\n",
    "Integer(1, 10, name='bagging_freq'),                  # 배깅 빈도\n",
    "Real(1e-9, 10.0, name='lambda_l1', prior='log-uniform'),    # L1 정규화\n",
    "Real(1e-9, 10.0, name='lambda_l2', prior='log-uniform'),    # L2 정규화\n",
    "Integer(5, 100, name='min_child_samples')             # 리프 노드의 최소 샘플 수\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272cb9e-b20e-4692-8f2a-224ed422fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# 하이퍼파라미터 이름\n",
    "param_names = [\n",
    "        \"n_estimators\",\n",
    "        \"learning_rate\", \n",
    "        \"num_leaves\",\n",
    "        \"max_depth\",\n",
    "        \"feature_fraction\",\n",
    "        \"bagging_fraction\",\n",
    "        \"bagging_freq\",\n",
    "        \"lambda_l1\",\n",
    "        \"lambda_l2\",\n",
    "        \"min_child_samples\"\n",
    "    ]\n",
    "datasets = {\n",
    "    'base':  (x_train_base,  x_test_base,  y_train_base,  y_test_base),\n",
    "    'var1':  (x_train_var1,  x_test_var1,  y_train_var1,  y_test_var1),\n",
    "    'var2':  (x_train_var2,  x_test_var2,  y_train_var2,  y_test_var2),\n",
    "    'var3':  (x_train_var3,  x_test_var3,  y_train_var3,  y_test_var3),\n",
    "    'var4':  (x_train_var4,  x_test_var4,  y_train_var4,  y_test_var4),\n",
    "    'var5':  (x_train_var5,  x_test_var5,  y_train_var5,  y_test_var5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5ece8-554e-475e-bc61-d0eaabef89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "roc_data = defaultdict(dict)\n",
    "pr_data = defaultdict(dict)\n",
    "threshold_metrics_data = defaultdict(dict)\n",
    "\n",
    "for name, (X_tr, X_te, y_tr, y_te) in datasets.items():\n",
    "    print(f\"\\n==== Processing dataset: {name} ====\")\n",
    "    def objective(params):     \n",
    "        param_dict = dict(zip(param_names, params))\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            **param_dict,\n",
    "            random_state=42, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='auc',\n",
    "            verbosity=-1\n",
    "            )\n",
    "\n",
    "        auc_scores, _, _, _ = k_fold_training_smote(model, X_tr, y_tr)\n",
    "        return -np.mean(auc_scores)\n",
    "    # 베이지안 최적화 수행\n",
    "    res = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=lgb_space,\n",
    "        n_calls=50,\n",
    "        n_random_starts=10,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # N개의 상위 조합을 저장\n",
    "    top_n = 15  # 원하는 개수\n",
    "    all_params = pd.DataFrame(res.x_iters, columns=param_names)\n",
    "    all_params['mean_cv_auc'] = -res.func_vals  # 최적화 목적이 -AUC였으므로 다시 양수로\n",
    "\n",
    "    top_params = all_params.sort_values(by='mean_cv_auc', ascending=False).head(top_n)\n",
    "\n",
    "    print(f\"\\n== 상위 {top_n}개 하이퍼파라미터 조합과 성능 ==\")\n",
    "    print(top_params)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    best_params = dict(zip(param_names, res.x))\n",
    "    print(\"  최적의 하이퍼파라미터:\", best_params)\n",
    "    print(f\"  CV AUC (평균): {-res.fun:.4f}\")\n",
    "    print(f\"최적의 ROC AUC (K-Fold 검증 평균): {-res.fun:.4f}\")\n",
    "\n",
    "\n",
    "# SMOTE + LGBMClassifier Pipeline\n",
    "    final_pipeline = Pipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', LGBMClassifier(\n",
    "            **best_params,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='auc',\n",
    "            verbosity=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 학습 데이터에 대해서만 SMOTE 적용하여 학습\n",
    "    final_pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "    # 중요도 추출\n",
    "    importance_dict = dict(zip(X_tr.columns, final_pipeline.named_steps['classifier'].feature_importances_))\n",
    "\n",
    "    # 보기 좋게 정렬\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': list(importance_dict.keys()),\n",
    "        'importance': list(importance_dict.values())\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    print(importance_df)\n",
    "\n",
    "    # 매핑 딕셔너리\n",
    "    fmap = {f\"f{i}\": col for i, col in enumerate(X_tr.columns)}\n",
    "\n",
    "    # 변환\n",
    "    importance_df['feature'] = importance_df['feature'].map(fmap)\n",
    "\n",
    "    # 베이지안 최적화된 모델로 예측\n",
    "    y_pred_label = final_pipeline.predict(X_te)\n",
    "    y_pred_proba = final_pipeline.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    auc_score   = roc_auc_score(y_te, y_pred_proba)\n",
    "    conf_mat    = confusion_matrix(y_te, y_pred_label)\n",
    "    acc = accuracy_score(y_te, y_pred_label)\n",
    "    prec = precision_score(y_te, y_pred_label)\n",
    "    rec = recall_score(y_te, y_pred_label)\n",
    "    fpr, tpr, thr = roc_curve(y_te, y_pred_proba)\n",
    "\n",
    "    print(f\"  테스트 AUC: {auc_score:.4f}, 정확도: {acc:.4f}, 정밀도: {prec:.4f}, 재현율: {rec:.4f}\")\n",
    "\n",
    "    # PR 곡선 좌표 저장\n",
    "    precision_pr, recall_pr, thresholds_pr = precision_recall_curve(y_te, y_pred_proba)\n",
    "    pr_auc = auc(recall_pr, precision_pr) # PR AUC 계산\n",
    "\n",
    "    pr_data[name]['precision'] = precision_pr\n",
    "    pr_data[name]['recall'] = recall_pr\n",
    "    pr_data[name]['auc'] = pr_auc # PR AUC 저장\n",
    "\n",
    "    \n",
    "    # roc_curve의 thresholds를 사용하여 임계값 배열을 가져옵니다.\n",
    "    # thresholds는 내림차순으로 정렬되어 있습니다.\n",
    "    _, _, thresholds_for_metrics = roc_curve(y_te, y_pred_proba)\n",
    "\n",
    "    # 고유한 임계값만 사용 (또는 일정 간격으로 샘플링 가능)\n",
    "    unique_thresholds = np.unique(thresholds_for_metrics)\n",
    "    # 임계값을 오름차순으로 정렬하여 그래프에서 0부터 1까지 순서대로 표시되게 합니다.\n",
    "    unique_thresholds = np.sort(unique_thresholds)\n",
    "\n",
    "    accuracies_at_threshold = []\n",
    "    precisions_at_threshold = []\n",
    "    recalls_at_threshold = []\n",
    "\n",
    "    for threshold in unique_thresholds:\n",
    "        y_pred_at_this_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "        accuracies_at_threshold.append(accuracy_score(y_te, y_pred_at_this_threshold))\n",
    "        precisions_at_threshold.append(precision_score(y_te, y_pred_at_this_threshold, zero_division=0))\n",
    "        recalls_at_threshold.append(recall_score(y_te, y_pred_at_this_threshold, zero_division=0))\n",
    "\n",
    "    threshold_metrics_data[name] = {\n",
    "        'thresholds': unique_thresholds, \n",
    "        'accuracy': accuracies_at_threshold, \n",
    "        'precision': precisions_at_threshold, \n",
    "        'recall': recalls_at_threshold\n",
    "        }\n",
    "\n",
    "\n",
    "    # 최적의 임계값 찾기 (예시: Youden's J statistic 최대화)\n",
    "    # 'thresholds' 배열은 fpr과 tpr이 계산된 임계값들을 포함합니다.\n",
    "    # thresholds[0]은 실제 사용된 가장 높은 임계값이며, thresholds[-1]은 가장 낮은 임계값입니다.\n",
    "    # 일반적으로 thresholds 배열은 내림차순으로 정렬되어 있습니다.\n",
    "    \n",
    "    youden_j_scores = tpr - fpr\n",
    "    youden_idx = np.argmax(youden_j_scores)\n",
    "    threshold_youden = thr[youden_idx]\n",
    "    fpr_youden = fpr[youden_idx]\n",
    "    tpr_youden = tpr[youden_idx]\n",
    "    print(\"\\nClassification Report with Optimal Threshold (Youden's J):\")\n",
    "    print(f\"최적 임계값 (Youden's J): {threshold_youden:.4f}\")\n",
    "\n",
    "    # 선택된 임계값으로 예측 수행\n",
    "    y_pred_custom_threshold = (y_pred_proba >= threshold_youden).astype(int)\n",
    "    print(classification_report(y_te, y_pred_custom_threshold))\n",
    "    \n",
    "    # 정확도 (Accuracy)\n",
    "    accuracy = accuracy_score(y_te, y_pred_custom_threshold)\n",
    "\n",
    "    # 정밀도 (Precision)\n",
    "    precision = precision_score(y_te, y_pred_custom_threshold, pos_label=1, zero_division=0)\n",
    "\n",
    "    # 재현율 (Recall)\n",
    "    recall = recall_score(y_te, y_pred_custom_threshold, pos_label=1, zero_division=0)\n",
    "    \n",
    "    print(\"\\n--- 최적 임계값 적용 시 성능 지표 ---\")\n",
    "    print(f\"정확도 (Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"정밀도 (Precision): {precision:.4f}\")\n",
    "    print(f\"재현율 (Recall): {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895eb07-8e32-4ebf-8aa7-eb8de4deac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "\n",
    "# [ROC Curve 통합 시각화]\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, res in roc_data.items():\n",
    "    plt.plot(res['fpr'], res['tpr'], label=f\"{name} (AUC={res['auc']:.4f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Variants별 ROC Curve 비교\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a951d-fef9-464f-8cc4-63d93a0baccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# PR Curve 통합 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, res in pr_data.items():\n",
    "    plt.plot(res['recall'], res['precision'], label=f\"{name} (AP={res['auc']:.4f})\") # AP = Average Precision, PR AUC와 동일\n",
    "\n",
    "plt.xlabel(\"Recall (재현율)\")\n",
    "plt.ylabel(\"Precision (정밀도)\")\n",
    "plt.title(\"Variants별 Precision-Recall Curve 비교\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋의 Accuracy를 한 그래프에 비교\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name, data in threshold_metrics_data.items():\n",
    "    plt.plot(data['thresholds'], data['accuracy'], label=f'{name} Accuracy')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Threshold Across Variants')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋의 Precision을 한 그래프에 비교\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name, data in threshold_metrics_data.items():\n",
    "    plt.plot(data['thresholds'], data['precision'], label=f'{name} Precision')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs. Threshold Across Variants')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋의 Recall을 한 그래프에 비교\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name, data in threshold_metrics_data.items():\n",
    "    plt.plot(data['thresholds'], data['recall'], label=f'{name} Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall vs. Threshold Across Variants')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
